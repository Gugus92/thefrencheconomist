<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>Comment stopper la « lethal trifecta » de l’IA</title>
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            margin: 40px;
            line-height: 1.6;
            color: #333;
        }
        h1 {
            font-size: 2em;
            margin-bottom: 0.2em;
        }
        h2 {
            font-size: 1.4em;
            margin-top: 0;
            color: #666;
        }
        p {
            margin-top: 1em;
        }
    </style>
</head>
<body>
    <h1>Comment stopper la « lethal trifecta » de l’IA</h1>
    <h2>Les codeurs doivent commencer à penser comme des ingénieurs mécaniciens.</h2>
    <p>Les modèles de langage étendu (MEL), une manière à la mode de construire l'intelligence artificielle, présentent un problème de sécurité inhérent : ils ne parviennent pas à séparer le code des données. Par conséquent, ils sont exposés à un type d'attaque appelé injection d'invite, qui consiste à les amener à suivre des commandes qu'ils ne devraient pas suivre. Parfois, le résultat se limite à une situation embarrassante, comme lorsque, pour faire le client-service, on persuade l'agent de parler comme un pirate. Dans d'autres cas, les conséquences sont bien plus dommageables.</p>
    <p>Les effets les plus graves de cette lacune sont réservés à ceux qui créent ce que l'on appelle la « trinité mortelle ». Si une entreprise, désireuse d’offrir un assistant IA puissant à ses employés, donne à un LLM accès à des données non fiables, la capacité de lire des secrets précieux et la capacité de communiquer avec le monde extérieur en même temps, des problèmes sont inévitables. Et éviter cela ne relève pas uniquement des ingénieurs en IA. Les utilisateurs ordinaires, eux aussi, doivent apprendre à utiliser l’IA en toute sécurité, car l’installation de la mauvaise combinaison d’applications peut générer la trinité mortelle accidentellement.</p>
    <p>Une meilleure ingénierie de l'IA constitue néanmoins la première ligne de défense. Et cela signifie que les ingénieurs en IA doivent commencer à penser comme des ingénieurs, qui construisent des ouvrages tels que des ponts et savent donc qu'un travail bâclé peut coûter des vies.</p>
    <p>Les grandes œuvres de l’Angleterre victorienne ont été érigées par des ingénieurs qui ne pouvaient être sûrs des propriétés des matériaux qu’ils utilisaient. En particulier, que ce soit par incompétence ou malhonnêteté, le fer de l’époque n’était souvent pas à la hauteur. Par conséquent, les ingénieurs privilégiaient la prudence, surdimensionnant leurs créations pour intégrer une redondance. Le résultat fut une série de chefs-d’œuvre qui traversent les siècles.</p>
    <p>Les fournisseurs de sécurité liés à l'IA ne pensent pas ainsi. La programmation conventionnelle est une pratique déterministe. Les vulnérabilités de sécurité sont considérées comme des erreurs à corriger, et une fois corrigées, elles disparaissent. Les ingénieurs en IA, imprégnés de cette manière de penser dès leur scolarité, agissent donc souvent comme si les problèmes pouvaient être résolus simplement avec davantage de données d'apprentissage et d'invites système plus pertinentes.</p>
    <p>Ils le font, en effet, réduire les risques. Les modèles frontaliers les plus performants sont meilleurs pour détecter et refuser les requêtes malveillantes que leurs prédécesseurs plus anciens ou moins puissants. Mais ils ne peuvent éliminer les risques entièrement. Contrairement à la plupart des logiciels, les LLM sont probabilistes. Leur production est motivée par une sélection aléatoire parmi les réponses les plus probables. Une approche déterministe de la sécurité est donc inadéquate. Une meilleure voie à suivre est d'imiter les ingénieurs du monde physique et d'apprendre à travailler avec, plutôt qu'en opposition, à des systèmes capricieux qui ne peuvent être garantis de fonctionner comme ils le devraient. Cela signifie devenir plus à l'aise avec l'imprévisibilité en introduisant des marges de sécurité, une tolérance au risque et des taux d'erreur.</p>
    <p>La surconstruction à l'ère de l'IA pourrait, par exemple, signifier l'utilisation d'un modèle plus puissant que nécessaire pour la tâche en question, afin de réduire le risque qu'il soit manipulé pour effectuer une action inappropriée. Cela pourrait signifier d'imposer des limites au nombre de requêtes que les LLM peuvent recevoir depuis des sources externes, ajustées au risque de dommages causés par une requête malveillante. Et la mécanique insiste sur la sécurité en cas de défaillance. Si un système d'IA doit avoir accès à des secrets, il faut éviter de lui donner les clés du royaume.</p>
    <p>Dans le monde physique, les ponts ont des limites de charge—même si elles ne sont pas toujours clairement indiquées aux conducteurs. Et, surtout, ces limites sont bien dans les marges de sécurité que les calculs suggèrent qu'un pont peut supporter. Le moment est venu d'équiper de la même manière le monde virtuel des systèmes d'IA.</p>
</body>
</html>