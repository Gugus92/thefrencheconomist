<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>Vidéo IA : bien plus que du « charabia »</title>
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            margin: 40px;
            line-height: 1.6;
            color: #333;
        }
        h1 {
            font-size: 2em;
            margin-bottom: 0.2em;
        }
        h2 {
            font-size: 1.4em;
            margin-top: 0;
            color: #666;
        }
        p {
            margin-top: 1em;
        }
    </style>
</head>
<body>
    <h1>Vidéo IA : bien plus que du « charabia »</h1>
    <h2>La prochaine grande avancée en IA pourrait être les images, et non les mots.</h2>
    <p>Faire défiler le fil d'actualité sur Sora, une nouvelle application vidéo développée par le créateur de chatbots Open AI, est une expérience hallucinatoire. Une femme en kimono de judo s'incline devant un éléphant avant de le lancer par-dessus son épaule. Une jeune patineuse artistique traverse les anneaux de Saturne. Des images granuleuses issues d'une caméra de sécurité capturent Sam Altman, le fondateur et le patron d'Open AI, tentant de cambrioler une carte graphique.</p>
    <p>Ce service, semblable à TikTok, serait un projet incongru pour une entreprise spécialisée dans l'IA, si ce n'était pour le fait que les vidéos sur Sora sont toutes générées par IA. Il n'y a pas la possibilité de télécharger ses propres images, ni même d'allumer sa caméra (à part pour activer une fonction qui insère son propre visage dans le générateur de vidéos IA). Le fil d'actualité de Sora n'est que du contenu de faible qualité, un nutriment insipide généré par IA, constamment. Les modèles vidéo, comme l'IA Sora sur laquelle l'application est construite, sont ce qui excite maintenant l'industrie de l'IA, alors que l'intérêt pour le texte s'estompe, et non seulement à cause de leur impact sur les médias de masse.</p>
    <p>Il ne faut pas croire que cet impact est minime. Bien qu’exclusivement sur invitation, l’application est en tête des classements des boutiques d’applications américaines et canadiennes, ses sites de lancement initiaux. Les codes d’“invitation” sont eux-mêmes devenus des produits de valeur, se vendant sur eBay entre 5 et 35 dollars. Lors de son lancement, elle a été suivie dans les classements par l’application Gemini de Google, elle-même ayant bénéficié d’un regain d’intérêt alimenté par le générateur d’images “Nano Banana” de l’entreprise. Les utilisateurs demandent au système de leur photo dans le style du personnage principal d’un film d’horreur des années 90, ou en train de se faire un câlin comme enfant, ou quelque chose d’également improbable, et il s’exécute avec diligence.</p>
    <p>Le succès a un prix. Pour ceux qui ont la chance de recevoir une invitation, Sora est utilisable gratuitement. Mais il n'est certainement pas gratuit à faire fonctionner. Chaque vidéo générée sur son site est estimée à environ 1 $ de puissance de calcul pour OpenAI, sur la base des prix de la première version de Sora, et les utilisateurs peuvent en générer 100 par jour. Le génie des médias sociaux était que les utilisateurs publiaient du contenu sans avoir besoin d’être payés et que les annonceurs payaient pour de l’espace à côté du leur. L’économie d’une application vidéo est un peu moins prometteuse si l’entreprise perd de l’argent à chaque publication.</p>
    <p>Mais la véritable valeur de Sora, et de modèles vidéo similaires comme Veo 3 de Google, ne réside probablement pas dans le travail bâclé qu'ils peuvent produire—même si celui-ci attire l'attention des utilisateurs. Au contraire, un article récent de chercheurs de Google DeepMind soutient que ces systèmes sont capables de résoudre un éventail de problèmes visuels et spatiaux sans aucun entraînement spécifique.</p>
    <p>Les modèles vidéo fonctionnent en prenant un bruit visuel aléatoire et en le « désactivant » progressivement, en y ajoutant de l'ordre au chaos. À chaque étape, ils se demandent : « Qu'est-ce qui ferait que cela ressemble davantage à l'instruction qui m'a été donnée ? » Si cette instruction décrit un contenu partageable, c'est ce que le modèle produira. Si elle décrit une tâche visuelle, comme la manipulation d'images ou la résolution de problèmes dans le monde réel, il s'avère que la dernière génération de modèles vidéo peut les résoudre également.</p>
    <p>Donnez-lui une image d'un perroquet sur un arbre et une requête demandant au modèle de produire une vidéo montrant toutes les couleurs et les détails s'estomper, ne laissant que les contours visibles, et il acceptera volontiers—effectuant un travail compétent de détection des contours, une tâche qui nécessitait auparavant des systèmes spécialisés. Une requête similaire peut le voir tenter de déflouter une image, à la manière de CSI, ou d'identifier ses parties constituantes.</p>
    <p>Il peut également prendre en charge des tâches qui diffèrent considérablement de la retouche d'images. Donnez-lui un puzzle sudoku inachevé et une requête décrivant une vidéo de la résolution du puzzle, et le modèle peut le faire. Une photographie de mains de robot tenant un bocal peut être étendue en une vidéo complète des mouvements que ces mains effectueraient pour ouvrir ce bocal.</p>
    <p>L'étendue des tâches que ces modèles peuvent effectuer fait d'eux, selon l'article, des « raisonneurs sans exemples ». Sans exemples car les systèmes vidéo peuvent résoudre des tâches qu'ils n'ont jamais vues auparavant, et pour lesquelles ils n'ont pas été explicitement formés. Raisonneurs car, au moins parfois, ils semblent bénéficier de ce que les chercheurs appellent le « raisonnement visuel par chaînes d'images », résolvant des tâches comme trouver la sortie d'un labyrinthe étape par étape.</p>
    <p>Prometteusement, le document précise que les nouveaux systèmes surpassent significativement les modèles vidéo de la génération précédente pour ce type de résolution de problèmes généralisée. Les auteurs suggèrent que cela signifie que les modèles vidéo « deviendront des modèles fondamentaux à usage général pour la vision » dans un futur proche, étant ultimement capables de résoudre n’importe quel défi visuel qui leur est posé, sans formation spécifique.</p>
    <p>C'est une affirmation audacieuse, mais qui trouve un écho historique. En 2022, une équipe de chercheurs de Google et de l'Université de Tokyo a publié un article notant que « les grands modèles linguistiques sont des raisonneurs à tir zéro », arguant que le domaine alors naissant des LLM disposait de « capacités fondamentales à tir zéro inexploitées et sous-étudiées ». Six mois plus tard, Chat GPT est arrivé et l'essor de l'IA a débuté. L'espoir est que les modèles vidéo mûriront avec une vague d'enthousiasme similaire—et que la phase de rodage de Sora se révélera alors une note intéressante dans leur développement, plutôt que l'élément essentiel. ■</p>
</body>
</html>